{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "681bea70-8a75-46af-aaed-b9726b4d0bda",
    "_uuid": "d438dd177f790e46e2bbf82574fb7d63b1416669"
   },
   "source": [
    "<a id=\"capsnet_model\"></a>\n",
    "### Capsule Network Model\n",
    "The Architecture of our CapsNet is very similar to general architecture, except for an addition Capsule Layer.\n",
    "\n",
    "\n",
    "\n",
    "#### Advantage of Capsule Layer in Text Classification\n",
    "As you can see, we have used Capsule layer instead of Pooling layer. Capsule Layer eliminates the need for forced pooling layers like MaxPool. In many cases, this is desired because we get translational invariance without losing minute details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten\n",
    "from keras.layers import concatenate, GRU, Input, K, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4758b588-6cf9-41d9-97f6-85ea9c7e59ab",
    "_uuid": "88116eab2cbc77c61dc4776d40e01f5ef5c62f5c"
   },
   "source": [
    "#### CapsNet parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.3\n",
    "rate_drop_dense = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "46eb605f-9281-4171-82da-ed4f541db6b6",
    "_uuid": "8be15963a074dde02ace1a1d3ca103bb28f2eb26"
   },
   "source": [
    "#### Capsule Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "## Plot\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib as plt\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from New_Utils import *\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, LSTM\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import *\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import *\n",
    "import tensorflow as tf\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ID  \\\n",
      "0    2017-En-21441   \n",
      "1    2017-En-31535   \n",
      "2    2017-En-21068   \n",
      "3    2017-En-31436   \n",
      "4    2017-En-22195   \n",
      "5    2017-En-22190   \n",
      "6    2017-En-20221   \n",
      "7    2017-En-22180   \n",
      "8    2017-En-41344   \n",
      "9    2017-En-20759   \n",
      "10   2017-En-40158   \n",
      "11   2017-En-21744   \n",
      "12   2017-En-31337   \n",
      "13   2017-En-10902   \n",
      "14   2017-En-11274   \n",
      "15   2017-En-11097   \n",
      "16   2017-En-11475   \n",
      "17   2017-En-10989   \n",
      "18   2017-En-10618   \n",
      "19   2017-En-40212   \n",
      "20   2017-En-21011   \n",
      "21   2017-En-20349   \n",
      "22   2017-En-40483   \n",
      "23   2017-En-30807   \n",
      "24   2017-En-21400   \n",
      "25   2017-En-11184   \n",
      "26   2017-En-30230   \n",
      "27   2017-En-11421   \n",
      "28   2017-En-21740   \n",
      "29   2017-En-10966   \n",
      "..             ...   \n",
      "970  2017-En-40939   \n",
      "971  2017-En-20749   \n",
      "972  2017-En-10495   \n",
      "973  2017-En-21856   \n",
      "974  2017-En-31286   \n",
      "975  2017-En-41417   \n",
      "976  2017-En-10375   \n",
      "977  2017-En-40332   \n",
      "978  2017-En-31312   \n",
      "979  2017-En-30923   \n",
      "980  2017-En-20143   \n",
      "981  2017-En-40754   \n",
      "982  2017-En-10791   \n",
      "983  2017-En-21471   \n",
      "984  2017-En-21652   \n",
      "985  2017-En-21991   \n",
      "986  2017-En-21274   \n",
      "987  2017-En-40020   \n",
      "988  2017-En-41323   \n",
      "989  2017-En-30858   \n",
      "990  2017-En-10326   \n",
      "991  2017-En-30212   \n",
      "992  2017-En-10515   \n",
      "993  2017-En-11234   \n",
      "994  2017-En-41063   \n",
      "995  2017-En-40077   \n",
      "996  2017-En-20969   \n",
      "997  2017-En-22069   \n",
      "998  2017-En-10351   \n",
      "999  2017-En-10716   \n",
      "\n",
      "                                                                                                                                                Tweet  \\\n",
      "0    â€œWorry is a down payment on a problem you may never have'. Â Joyce Meyer.  #motivation #leadership #worry                                           \n",
      "1    Whatever you decide to do make sure it makes you #happy.                                                                                           \n",
      "2    @Max_Kellerman  it also helps that the majority of NFL coaching is inept. Some of Bill O'Brien's play calling was wow, ! #GOPATS                   \n",
      "3    Accept the challenges so that you can literally even feel the exhilaration of victory.' -- George S. Patton ðŸ¶                                      \n",
      "4    My roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs                                                 \n",
      "5    No but that's so cute. Atsu was probably shy about photos before but cherry helped her out uwu                                                     \n",
      "6    Do you think humans have the sense for recognizing impending doom?                                                                                 \n",
      "7    Rooneys fucking untouchable isn't he? Been fucking dreadful again, depay has looked decent(ish)tonight                                             \n",
      "8    it's pretty depressing when u hit pan on ur favourite highlighter                                                                                  \n",
      "9    @BossUpJaee but your pussy was weak from what I heard so stfu up to me bitch . You got to threaten him that your pregnant .                        \n",
      "10   Making that yearly transition from excited and hopeful college returner to sick and exhausted pessimist. #college                                  \n",
      "11   And it's hard to dance with a devil on your back\\nSo shake him off.                                                                                \n",
      "12   Tiller and breezy should do a collab album. Rapping and singing prolly be fire                                                                     \n",
      "13   S/O to the girl that just hit my car...not only did she get lucky w/ no scratch but also from being spared the wrath of sleep deprived KaitðŸ™ƒ       \n",
      "14   @bt_uk broadband is shocking regretting signing up now #angry #shouldofgonewithvirgin                                                              \n",
      "15   People you need to look up the definition of protest. What you are doing is not protesting is called vandalism. #angry #stop                       \n",
      "16   @BitchestheCat Look at those teef! #growl                                                                                                          \n",
      "17   Star trek online has a update to download oh fuming yay                                                                                            \n",
      "18   The bitter the battle, the sweeter the victory...                                                                                                  \n",
      "19   i cant stop. i finished - dejected. luckily no one is in the bathroom. so i go to a stall and wait until my pants are dry.                         \n",
      "20   @NHLexpertpicks @usahockey USA was embarrassing to watch. When was the last time you guys won a game..? #horrible #joke                            \n",
      "21   #NewYork: Several #Baloch &amp; Indian activists hold demonstrations outside @UN headquarters demanding Pak to stop exporting #terror into India   \n",
      "22   Awareness of time is awareness of time lost. #awareness #time                                                                                      \n",
      "23   Your glee filled Normy dry humping of the most recent high profile celebrity break up is pathetic &amp; all that is wrong with the world today.    \n",
      "24   @zorefx you guys will see what i did in the next video but its fucking horrid                                                                      \n",
      "25   What a fucking muppet.  @jRwild1  #stalker.                                                                                                        \n",
      "26   @adamrodricks I like your optimism!                                                                                                                \n",
      "27   My cat is bloody lucky the RSPCA weren't open at 3am last night!!! #fuming ðŸ˜¡ðŸ±                                                                      \n",
      "28   @FaithHill I remember it well #happy #afraid #Positive                                                                                             \n",
      "29   Autocorrect changes ''em' to 'me' which I resent greatly                                                                                           \n",
      "..                                                        ...                                                                                           \n",
      "970  Now I'm going to sulk again for a week until #OurGirl is back on! @michkeegan and @benaldridge07 are the best! @OurGirlWatch                       \n",
      "971  Big up @dj_argue, did I miss phantom dread syndrome by @GenesisElijah? dudes sick.                                                                 \n",
      "972  #welfarereform should not be a 'model' for #snap.                                                                                                  \n",
      "973  @makemytripcare but what I am doing is in my control, #AvoidMMT , you guys are                                                                     \n",
      "974  Haven't yet read why Dutch drivers are so happy, but here's my theory: because they could quit driving and still have great transpo options.       \n",
      "975  I truly feel like science has the ability to make a milk out of anything ... cashew milk, hemp milk, pine nut milk, dandelion milk                 \n",
      "976  @cc_yandian @HillaryClinton her team must draw from a hat for daily personality #drugged  #yeller #quiet #screamer #ðŸ˜‚ðŸ˜‚                             \n",
      "977  Regret for the things we did can be tempered by time; it is regret for the things we did not do that is inconsolable.  Sydney J. Harris            \n",
      "978  @davidplotz @Commentary She writes with sort of the same playful cynicism that you do! :)                                                          \n",
      "979  So happy I live in NYC! See you tomorrow .....@SamHeughan @Barbour                                                                                 \n",
      "980  # ISIS REFERENCES SCRUBBED?  Federal complaint against suspect in NYC, NJ bombings appears to omit terror names in bloody journ...  #news          \n",
      "981  Free live music in DC tonight!  #blues with #MoonshineSociety at @thehamiltondc in the Loft starting at 10:30pm @FreeinDCBlog @WTOPFreebies        \n",
      "982  ESPN just assumed I wanted their free magazines                                                                                                    \n",
      "983  It's amazing how something gets stuck in your head, and you can't shake a memory...  Sometimes, I miss people a lot.  Wish they knew.              \n",
      "984  @maddie_truex just wait till they hit you with the 5am fire alarm in the middle of winter. Those are sweet                                         \n",
      "985  I am a third year college student and and English major. Today is the first time I've ever written an essay without having a panic attack          \n",
      "986  @chelsysayshi @Adweek touchÃ©! I'll start paying more attention and test this ðŸ•µðŸ»â€â™€ï¸ #AdweekChat                                                     \n",
      "987  @Ashleyavitia_ don't leave me #sad                                                                                                                 \n",
      "988  Honestly don't know why I'm so unhappy most of the time. I just want it all to stop :(   #itnevergoes                                              \n",
      "989  It's #HobbitDay! \\nHobbit's give gifts on their birthdays, so I'm sharing the gift of #glee today! Find something that makes you happy!            \n",
      "990  I am #real #sjw, I will not let #america down\\nI have found us, now go and get us\\nI let it out and I let it in\\nI #rage I #lol\\n@SergioSarzedo    \n",
      "991  The neighbor dancing in the Clayton Homes commercial is me. #hilarious                                                                             \n",
      "992  @OstinOng YUUUHH ðŸ™„ðŸ˜­ plus clin ep and prevmed ugghhh hahaha                                                                                         \n",
      "993  @russbully Ended up paying 75p for half a tube of smarties. Don't even get the pleasure of popping the plastic lid off either #outrage             \n",
      "994  Rooney = whipping boy. #mufc                                                                                                                       \n",
      "995  @CTV_PowerPlay @lraitt Horrid disease! My maternal grandmother and each of her sisters suffered from this affliction. It's hard on all.            \n",
      "996  How is that in 2016, a 757 airplane does not have WiFi...ridiculous. #AmericanAirlines #americanairlinessucks #AATeam                              \n",
      "997  Side chick's be trying to fuck u like your going to forget about your wife when your done.                                                         \n",
      "998  @TillTheEndMMVI I accidetly dumped boiling water my myself ðŸ˜«                                                                                       \n",
      "999  Praying for the #Lord to keep #anger #hate #jealousy away from your heart is a sign of #maturity #conciseness                                      \n",
      "\n",
      "     anger  anticipation  disgust  fear  joy  love  optimism  pessimism  \\\n",
      "0    0      1             0        0     0    0     1         0           \n",
      "1    0      0             0        0     1    1     1         0           \n",
      "2    1      0             1        0     1    0     1         0           \n",
      "3    0      0             0        0     1    0     1         0           \n",
      "4    1      0             1        0     0    0     0         0           \n",
      "5    0      0             0        0     1    0     0         0           \n",
      "6    0      1             0        0     0    0     0         1           \n",
      "7    1      0             1        0     0    0     0         0           \n",
      "8    0      0             1        0     0    0     0         0           \n",
      "9    1      0             1        0     0    0     0         0           \n",
      "10   0      0             1        0     0    0     0         1           \n",
      "11   0      0             0        0     0    0     0         0           \n",
      "12   0      0             0        0     1    1     0         0           \n",
      "13   1      0             0        0     0    0     0         0           \n",
      "14   1      0             1        0     0    0     0         0           \n",
      "15   1      0             1        0     0    0     0         0           \n",
      "16   1      0             1        1     0    0     0         0           \n",
      "17   1      0             1        0     1    0     0         0           \n",
      "18   0      0             0        0     1    0     1         0           \n",
      "19   1      0             1        0     0    0     0         0           \n",
      "20   1      0             1        0     0    0     0         0           \n",
      "21   0      0             0        1     0    0     0         0           \n",
      "22   0      0             0        0     1    1     1         0           \n",
      "23   1      0             1        0     0    0     0         0           \n",
      "24   1      0             1        1     0    0     0         0           \n",
      "25   1      0             1        0     0    0     0         0           \n",
      "26   0      0             0        0     1    0     1         0           \n",
      "27   1      0             1        0     1    0     0         0           \n",
      "28   0      0             0        1     1    1     1         0           \n",
      "29   1      0             1        0     0    0     0         0           \n",
      "..  ..     ..            ..       ..    ..   ..    ..        ..           \n",
      "970  1      1             0        0     0    0     0         0           \n",
      "971  1      0             1        0     0    0     0         0           \n",
      "972  0      0             1        0     0    0     0         0           \n",
      "973  1      0             1        0     0    0     0         0           \n",
      "974  0      0             0        0     1    0     1         0           \n",
      "975  0      1             0        0     0    0     1         0           \n",
      "976  1      0             1        0     0    0     0         0           \n",
      "977  0      0             0        0     0    0     1         0           \n",
      "978  0      1             0        0     0    0     0         0           \n",
      "979  0      0             0        0     1    0     1         0           \n",
      "980  1      1             1        0     0    0     0         0           \n",
      "981  0      1             0        0     1    0     0         0           \n",
      "982  1      0             1        0     0    0     0         0           \n",
      "983  0      0             0        0     1    0     0         0           \n",
      "984  1      1             1        0     0    0     1         0           \n",
      "985  0      0             0        1     1    0     0         0           \n",
      "986  0      0             0        0     1    0     1         0           \n",
      "987  0      0             0        0     0    0     0         1           \n",
      "988  0      0             0        1     0    0     0         1           \n",
      "989  0      0             0        0     1    1     1         0           \n",
      "990  1      0             0        0     0    0     0         0           \n",
      "991  0      0             0        0     1    0     0         0           \n",
      "992  0      0             0        0     1    0     0         0           \n",
      "993  1      0             1        0     0    0     0         0           \n",
      "994  0      0             0        0     1    0     0         0           \n",
      "995  0      0             1        1     0    0     0         1           \n",
      "996  1      0             1        0     0    0     0         0           \n",
      "997  1      0             1        0     0    0     0         0           \n",
      "998  1      0             1        0     0    0     0         0           \n",
      "999  0      0             0        0     0    0     1         0           \n",
      "\n",
      "     sadness surprise  trust Tweet_label  \n",
      "0    0        0        1      Exp         \n",
      "1    0        0        0      Sug         \n",
      "2    0        0        0      Exp         \n",
      "3    0        0        0      Sug         \n",
      "4    0        0        0      Sta         \n",
      "5    0        0        0      Sta         \n",
      "6    0        0        0      Que         \n",
      "7    0        0        0      Exp         \n",
      "8    1        0        0      Sta         \n",
      "9    0        0        0      Thr         \n",
      "10   1        0        0      Sta         \n",
      "11   0        0        0      Sug         \n",
      "12   0        0        0      Sug         \n",
      "13   1        0        0      Sta         \n",
      "14   0        0        0      Sta         \n",
      "15   1        0        0      Exp         \n",
      "16   0        0        0      Sta         \n",
      "17   1        0        0      Sta         \n",
      "18   0        0        0      Exp         \n",
      "19   1        0        0      Sta         \n",
      "20   0        0        0      Que         \n",
      "21   1        0        0      Thr         \n",
      "22   0        0        0      Exp         \n",
      "23   0        0        0      Exp         \n",
      "24   1        0        0      Exp         \n",
      "25   0        0        0      Exp         \n",
      "26   0        0        0      Exp         \n",
      "27   0        0        0      Sta         \n",
      "28   0        0        0      Sta         \n",
      "29   0        0        0      Sta         \n",
      "..  ..       ..       ..      ...         \n",
      "970  1        0        0      sta         \n",
      "971  1        0        0      sta         \n",
      "972  1        0        0      exp         \n",
      "973  0        0        0      sta         \n",
      "974  0        0        0      exp         \n",
      "975  0        1        0      exp         \n",
      "976  0        0        0      sug         \n",
      "977  1        0        0      exp         \n",
      "978  0        0        0      exp         \n",
      "979  0        0        0      sta         \n",
      "980  0        0        0      thr         \n",
      "981  0        0        0      sta         \n",
      "982  0        0        0      sta         \n",
      "983  1        0        0      sta         \n",
      "984  1        0        0      sta         \n",
      "985  0        0        0      sta         \n",
      "986  0        0        0      sta         \n",
      "987  1        0        0      sta         \n",
      "988  1        0        0      sta         \n",
      "989  0        0        0      sta         \n",
      "990  0        0        0      sta         \n",
      "991  0        1        0      sta         \n",
      "992  0        0        0      oth         \n",
      "993  0        0        0      sta         \n",
      "994  0        0        0      oth         \n",
      "995  1        0        0      exp         \n",
      "996  1        0        0      que         \n",
      "997  0        0        0      exp         \n",
      "998  1        0        0      sta         \n",
      "999  0        0        0      sta         \n",
      "\n",
      "[1000 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"tweet_emot_train.csv\")\n",
    "print(data_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(248, ': '),\n",
       " (131, 'xt'),\n",
       " (65, ':)'),\n",
       " (29, ':('),\n",
       " (7, ';)'),\n",
       " (7, ':D'),\n",
       " (6, ':/'),\n",
       " (3, 'XD'),\n",
       " (2, ';-O'),\n",
       " (2, ';-)'),\n",
       " (2, ':-/'),\n",
       " (2, ':-('),\n",
       " (1, ':u'),\n",
       " (1, ':o'),\n",
       " (1, ':c'),\n",
       " (1, \":'D\")]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are gonna find what emoticons are used in our dataset\n",
    "import re\n",
    "tweets_text = data_train.Tweet.str.cat()\n",
    "emos = set(re.findall(r\" ([xX:;][-']?.) \",tweets_text))\n",
    "emos_count = []\n",
    "for emo in emos:\n",
    "    emos_count.append((tweets_text.count(emo), emo))\n",
    "sorted(emos_count,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy emoticons: {';)', ';-)', ':)', ':D', 'XD'}\n",
      "Sad emoticons: {':/', ':('}\n"
     ]
    }
   ],
   "source": [
    "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n",
    "SAD_EMO = r\" (:'?[/|\\(]) \"\n",
    "print(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\n",
    "print(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove punctuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    reformed=[]\n",
    "    HAPPY_SMILEYS= {'XD', ':)', ';)', ';-)', ':D'}\n",
    "    SAD_SMILEYS={':/', ':('}\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in SAD_SMILEYS:\n",
    "            reformed.append(\"sad\")\n",
    "        elif word in HAPPY_SMILEYS:\n",
    "            reformed.append(\"happy\")\n",
    "        else:\n",
    "            reformed.append(word)\n",
    "    text = \" \".join(reformed)\n",
    "    \n",
    "    text = emoji.demojize(text)\n",
    "    text = text.replace(\":\",\" \")\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Tweet'] = data_train['Tweet'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ID  \\\n",
      "0    2017-En-21441   \n",
      "1    2017-En-31535   \n",
      "2    2017-En-21068   \n",
      "3    2017-En-31436   \n",
      "4    2017-En-22195   \n",
      "5    2017-En-22190   \n",
      "6    2017-En-20221   \n",
      "7    2017-En-22180   \n",
      "8    2017-En-41344   \n",
      "9    2017-En-20759   \n",
      "10   2017-En-40158   \n",
      "11   2017-En-21744   \n",
      "12   2017-En-31337   \n",
      "13   2017-En-10902   \n",
      "14   2017-En-11274   \n",
      "15   2017-En-11097   \n",
      "16   2017-En-11475   \n",
      "17   2017-En-10989   \n",
      "18   2017-En-10618   \n",
      "19   2017-En-40212   \n",
      "20   2017-En-21011   \n",
      "21   2017-En-20349   \n",
      "22   2017-En-40483   \n",
      "23   2017-En-30807   \n",
      "24   2017-En-21400   \n",
      "25   2017-En-11184   \n",
      "26   2017-En-30230   \n",
      "27   2017-En-11421   \n",
      "28   2017-En-21740   \n",
      "29   2017-En-10966   \n",
      "..             ...   \n",
      "970  2017-En-40939   \n",
      "971  2017-En-20749   \n",
      "972  2017-En-10495   \n",
      "973  2017-En-21856   \n",
      "974  2017-En-31286   \n",
      "975  2017-En-41417   \n",
      "976  2017-En-10375   \n",
      "977  2017-En-40332   \n",
      "978  2017-En-31312   \n",
      "979  2017-En-30923   \n",
      "980  2017-En-20143   \n",
      "981  2017-En-40754   \n",
      "982  2017-En-10791   \n",
      "983  2017-En-21471   \n",
      "984  2017-En-21652   \n",
      "985  2017-En-21991   \n",
      "986  2017-En-21274   \n",
      "987  2017-En-40020   \n",
      "988  2017-En-41323   \n",
      "989  2017-En-30858   \n",
      "990  2017-En-10326   \n",
      "991  2017-En-30212   \n",
      "992  2017-En-10515   \n",
      "993  2017-En-11234   \n",
      "994  2017-En-41063   \n",
      "995  2017-En-40077   \n",
      "996  2017-En-20969   \n",
      "997  2017-En-22069   \n",
      "998  2017-En-10351   \n",
      "999  2017-En-10716   \n",
      "\n",
      "                                                                                                                                Tweet  \\\n",
      "0    worri payment problem may never have joyc meyer motiv leadership worri                                                             \n",
      "1    whatev decid make sure make happi                                                                                                  \n",
      "2    max kellerman also help major nfl coach inept bill o brien play call wow gopat                                                     \n",
      "3    accept challeng liter even feel exhilar victori georg patton                                                                       \n",
      "4    roommat okay ca not spell autocorrect terribl firstworldprob                                                                       \n",
      "5    that cute atsu probabl shi photo cherri help uwu                                                                                   \n",
      "6    think human sens recogn impend doom                                                                                                \n",
      "7    rooney fuck untouch he fuck dread again depay look decent ish tonight                                                              \n",
      "8    pretti depress hit pan favourit highlight                                                                                          \n",
      "9    bossupjae pussi weak heard stfu bitch got threaten pregnant                                                                        \n",
      "10   make year transit excit hope colleg return sick exhaust pessimist colleg                                                           \n",
      "11   hard danc devil back nso shake off                                                                                                 \n",
      "12   tiller breezi collab album rap sing prolli fire                                                                                    \n",
      "13   s o girl hit car not get lucki scratch also spare wrath sleep depriv kait upsid - down face                                        \n",
      "14   bt uk broadband shock regret sign angri shouldofgonewithvirgin                                                                     \n",
      "15   peopl need look definit protest protest call vandal angri stop                                                                     \n",
      "16   bitchesthecat look teef ! growl                                                                                                    \n",
      "17   star trek onlin updat download fume yay                                                                                            \n",
      "18   bitter battl sweeter victori                                                                                                       \n",
      "19   cant stop finish deject luckili one bathroom stall wait pant dri                                                                   \n",
      "20   nhlexpertpick usahockey usa embarrass watch last time guy game horribl joke                                                        \n",
      "21   newyork sever baloch amp; indian activist hold demonstr outsid un headquart demand pak stop export terror india                    \n",
      "22   awar time awar time lost awar time                                                                                                 \n",
      "23   glee fill normi dri hump recent high profil celebr break pathet amp; wrong world today                                             \n",
      "24   zorefx guy see next video fuck horrid                                                                                              \n",
      "25   fuck muppet jrwild1 stalker                                                                                                        \n",
      "26   adamrodrick like optim !                                                                                                           \n",
      "27   cat bloodi lucki rspca open 3am last night ! ! ! fume                                                                              \n",
      "28   faithhil rememb well happi afraid posit                                                                                            \n",
      "29   autocorrect chang em me resent great                                                                                               \n",
      "..                                    ...                                                                                               \n",
      "970  i am go sulk week ourgirl back on ! michkeegan benaldridge07 best ! ourgirlwatch                                                   \n",
      "971  big dj argu miss phantom dread syndrom genesiselijah dude sick                                                                     \n",
      "972  welfarereform model snap                                                                                                           \n",
      "973  makemytripcar control avoidmmt guy                                                                                                 \n",
      "974  yet read dutch driver happi here theori could quit drive still great transpo option                                                \n",
      "975  truli feel like scienc abil make milk anyth cashew milk hemp milk pine nut milk dandelion milk                                     \n",
      "976  cc yandian hillaryclinton team must draw hat daili person drug yeller quiet screamer face with tear of joy face with tear of joy   \n",
      "977  regret thing temper time; regret thing inconsol sydney harri                                                                       \n",
      "978  davidplotz commentari write sort play cynic do !                                                                                   \n",
      "979  happi live nyc ! see tomorrow samheughan barbour                                                                                   \n",
      "980  isi refer scrub feder complaint suspect nyc bomb appear omit terror name bloodi journ news                                         \n",
      "981  free live music tonight ! blue moonshinesocieti thehamiltondc loft start 10 30pm freeindcblog wtopfreebi                           \n",
      "982  espn assum want free magazin                                                                                                       \n",
      "983  amaz someth get stuck head ca not shake memori sometim miss peopl lot wish knew                                                    \n",
      "984  maddi truex wait till hit 5am fire alarm middl winter sweet                                                                        \n",
      "985  third year colleg student english major today first time i have ever written essay without panic attack                            \n",
      "986  chelsysayshi adweek touch ! i will start pay attent test woman detect light skin tone adweekchat                                   \n",
      "987  ashleyavitia leav sad                                                                                                              \n",
      "988  honest know i am unhappi time want stop itnevergo                                                                                  \n",
      "989  hobbitday ! nhobbit give gift birthday i am share gift glee today ! find someth make happi !                                       \n",
      "990  real sjw let america down ni found us get us ni let let in ni rage lol n sergiosarzedo                                             \n",
      "991  neighbor danc clayton home commerci me hilari                                                                                      \n",
      "992  ostinong yuuuhh plus clin prevm ugghhh hahaha                                                                                      \n",
      "993  russbulli end pay 75p half tube smarti even get pleasur pop plastic lid either outrag                                              \n",
      "994  rooney whip boy mufc                                                                                                               \n",
      "995  ctv powerplay lraitt horrid diseas ! matern grandmoth sister suffer afflict hard all                                               \n",
      "996  2016 757 airplan wifi ridicul americanairlin americanairlinessuck aateam                                                           \n",
      "997  side chick tri fuck like go forget wife done                                                                                       \n",
      "998  tilltheendmmvi accidet dump boil water                                                                                             \n",
      "999  pray lord keep anger hate jealousi away heart sign matur concis                                                                    \n",
      "\n",
      "     anger  anticipation  disgust  fear  joy  love  optimism  pessimism  \\\n",
      "0    0      1             0        0     0    0     1         0           \n",
      "1    0      0             0        0     1    1     1         0           \n",
      "2    1      0             1        0     1    0     1         0           \n",
      "3    0      0             0        0     1    0     1         0           \n",
      "4    1      0             1        0     0    0     0         0           \n",
      "5    0      0             0        0     1    0     0         0           \n",
      "6    0      1             0        0     0    0     0         1           \n",
      "7    1      0             1        0     0    0     0         0           \n",
      "8    0      0             1        0     0    0     0         0           \n",
      "9    1      0             1        0     0    0     0         0           \n",
      "10   0      0             1        0     0    0     0         1           \n",
      "11   0      0             0        0     0    0     0         0           \n",
      "12   0      0             0        0     1    1     0         0           \n",
      "13   1      0             0        0     0    0     0         0           \n",
      "14   1      0             1        0     0    0     0         0           \n",
      "15   1      0             1        0     0    0     0         0           \n",
      "16   1      0             1        1     0    0     0         0           \n",
      "17   1      0             1        0     1    0     0         0           \n",
      "18   0      0             0        0     1    0     1         0           \n",
      "19   1      0             1        0     0    0     0         0           \n",
      "20   1      0             1        0     0    0     0         0           \n",
      "21   0      0             0        1     0    0     0         0           \n",
      "22   0      0             0        0     1    1     1         0           \n",
      "23   1      0             1        0     0    0     0         0           \n",
      "24   1      0             1        1     0    0     0         0           \n",
      "25   1      0             1        0     0    0     0         0           \n",
      "26   0      0             0        0     1    0     1         0           \n",
      "27   1      0             1        0     1    0     0         0           \n",
      "28   0      0             0        1     1    1     1         0           \n",
      "29   1      0             1        0     0    0     0         0           \n",
      "..  ..     ..            ..       ..    ..   ..    ..        ..           \n",
      "970  1      1             0        0     0    0     0         0           \n",
      "971  1      0             1        0     0    0     0         0           \n",
      "972  0      0             1        0     0    0     0         0           \n",
      "973  1      0             1        0     0    0     0         0           \n",
      "974  0      0             0        0     1    0     1         0           \n",
      "975  0      1             0        0     0    0     1         0           \n",
      "976  1      0             1        0     0    0     0         0           \n",
      "977  0      0             0        0     0    0     1         0           \n",
      "978  0      1             0        0     0    0     0         0           \n",
      "979  0      0             0        0     1    0     1         0           \n",
      "980  1      1             1        0     0    0     0         0           \n",
      "981  0      1             0        0     1    0     0         0           \n",
      "982  1      0             1        0     0    0     0         0           \n",
      "983  0      0             0        0     1    0     0         0           \n",
      "984  1      1             1        0     0    0     1         0           \n",
      "985  0      0             0        1     1    0     0         0           \n",
      "986  0      0             0        0     1    0     1         0           \n",
      "987  0      0             0        0     0    0     0         1           \n",
      "988  0      0             0        1     0    0     0         1           \n",
      "989  0      0             0        0     1    1     1         0           \n",
      "990  1      0             0        0     0    0     0         0           \n",
      "991  0      0             0        0     1    0     0         0           \n",
      "992  0      0             0        0     1    0     0         0           \n",
      "993  1      0             1        0     0    0     0         0           \n",
      "994  0      0             0        0     1    0     0         0           \n",
      "995  0      0             1        1     0    0     0         1           \n",
      "996  1      0             1        0     0    0     0         0           \n",
      "997  1      0             1        0     0    0     0         0           \n",
      "998  1      0             1        0     0    0     0         0           \n",
      "999  0      0             0        0     0    0     1         0           \n",
      "\n",
      "     sadness surprise  trust Tweet_label  \n",
      "0    0        0        1      Exp         \n",
      "1    0        0        0      Sug         \n",
      "2    0        0        0      Exp         \n",
      "3    0        0        0      Sug         \n",
      "4    0        0        0      Sta         \n",
      "5    0        0        0      Sta         \n",
      "6    0        0        0      Que         \n",
      "7    0        0        0      Exp         \n",
      "8    1        0        0      Sta         \n",
      "9    0        0        0      Thr         \n",
      "10   1        0        0      Sta         \n",
      "11   0        0        0      Sug         \n",
      "12   0        0        0      Sug         \n",
      "13   1        0        0      Sta         \n",
      "14   0        0        0      Sta         \n",
      "15   1        0        0      Exp         \n",
      "16   0        0        0      Sta         \n",
      "17   1        0        0      Sta         \n",
      "18   0        0        0      Exp         \n",
      "19   1        0        0      Sta         \n",
      "20   0        0        0      Que         \n",
      "21   1        0        0      Thr         \n",
      "22   0        0        0      Exp         \n",
      "23   0        0        0      Exp         \n",
      "24   1        0        0      Exp         \n",
      "25   0        0        0      Exp         \n",
      "26   0        0        0      Exp         \n",
      "27   0        0        0      Sta         \n",
      "28   0        0        0      Sta         \n",
      "29   0        0        0      Sta         \n",
      "..  ..       ..       ..      ...         \n",
      "970  1        0        0      sta         \n",
      "971  1        0        0      sta         \n",
      "972  1        0        0      exp         \n",
      "973  0        0        0      sta         \n",
      "974  0        0        0      exp         \n",
      "975  0        1        0      exp         \n",
      "976  0        0        0      sug         \n",
      "977  1        0        0      exp         \n",
      "978  0        0        0      exp         \n",
      "979  0        0        0      sta         \n",
      "980  0        0        0      thr         \n",
      "981  0        0        0      sta         \n",
      "982  0        0        0      sta         \n",
      "983  1        0        0      sta         \n",
      "984  1        0        0      sta         \n",
      "985  0        0        0      sta         \n",
      "986  0        0        0      sta         \n",
      "987  1        0        0      sta         \n",
      "988  1        0        0      sta         \n",
      "989  0        0        0      sta         \n",
      "990  0        0        0      sta         \n",
      "991  0        1        0      sta         \n",
      "992  0        0        0      oth         \n",
      "993  0        0        0      sta         \n",
      "994  0        0        0      oth         \n",
      "995  1        0        0      exp         \n",
      "996  1        0        0      que         \n",
      "997  0        0        0      exp         \n",
      "998  1        0        0      sta         \n",
      "999  0        0        0      sta         \n",
      "\n",
      "[1000 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(data_train[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file_path = \"../input/donorschooseorg-preprocessed-data/train_preprocessed.csv\"\n",
    "train_file_path = \"tweet_emot_train.csv\"\n",
    "\n",
    "# embedding_path = \"../input/fastttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec\"\n",
    "embedding_path = \"crawl-300d-2M.vec\"\n",
    "\n",
    "batch_size = 128 # 256\n",
    "recurrent_units = 16 # 64\n",
    "dropout_rate = 0.3 \n",
    "dense_size = 8 # 32\n",
    "fold_count = 2 # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "print\n",
    "train = [s.strip() for s in data_train['Tweet']]\n",
    "\n",
    "text = train\n",
    "max1=0\n",
    "\n",
    "for i in range(0,len(text)):\n",
    "\tdata=text[i].split(\" \")\n",
    "\tmax2=len(data)\n",
    "\tif(max2>max1):\n",
    "\t\tmax1=max2\n",
    "\n",
    "\n",
    "sequences_length = max1\n",
    "sentences_length = sequences_length \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp': 0, 'sug': 1, 'sta': 2, 'que': 3, 'thr': 4, 'oth': 5, 'req': 6, 'sat': 7, 'sst': 8, 'aue': 9, 'ue': 10, 'ot': 11, 'sa': 12}\n",
      "0       exp\n",
      "1       sug\n",
      "2       exp\n",
      "3       sug\n",
      "4       sta\n",
      "5       sta\n",
      "6       que\n",
      "7       exp\n",
      "8       sta\n",
      "9       thr\n",
      "10      sta\n",
      "11      sug\n",
      "12      sug\n",
      "13      sta\n",
      "14      sta\n",
      "15      exp\n",
      "16      sta\n",
      "17      sta\n",
      "18      exp\n",
      "19      sta\n",
      "20      que\n",
      "21      thr\n",
      "22      exp\n",
      "23      exp\n",
      "24      exp\n",
      "25      exp\n",
      "26      exp\n",
      "27      sta\n",
      "28      sta\n",
      "29      sta\n",
      "       ... \n",
      "6808    oth\n",
      "6809    oth\n",
      "6810    que\n",
      "6811    exp\n",
      "6812    sug\n",
      "6813    sta\n",
      "6814    sta\n",
      "6815    exp\n",
      "6816    sta\n",
      "6817    exp\n",
      "6818    exp\n",
      "6819    exp\n",
      "6820    sta\n",
      "6821    que\n",
      "6822    sta\n",
      "6823    exp\n",
      "6824    que\n",
      "6825    exp\n",
      "6826    exp\n",
      "6827    que\n",
      "6828    sug\n",
      "6829    oth\n",
      "6830    oth\n",
      "6831    oth\n",
      "6832    que\n",
      "6833    sta\n",
      "6834    sta\n",
      "6835    sug\n",
      "6836    que\n",
      "6837    sta\n",
      "Name: Tweet_label, Length: 6838, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_train['Tweet_label']=data_train['Tweet_label'].str.lower()\n",
    "\n",
    "train_label=data_train['Tweet_label']\n",
    "lbl_dict={}\n",
    "index=0\n",
    "for dial_lbls in train_label:\n",
    "\tif dial_lbls not in lbl_dict:\n",
    "\t\tlbl_dict[dial_lbls]=index\n",
    "\t\tindex=index+1\n",
    "\n",
    "print(lbl_dict)\n",
    "print(data_train['Tweet_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil Aggarwal\\Desktop\\Speech Act Classification\\New_Utils.py:40: FutureWarning:\n",
      "\n",
      "arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer=load_create_tokenizer(data_train['ID'],None,True)\n",
    "X_train=load_create_padded_data(X_train=train,savetokenizer=False,isPaddingDone=False,maxlen=sequences_length,tokenizer_path='./New_Tokenizer.tkn')\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix=load_create_embedding_matrix(word_index,len(word_index)+1,300,'glove.6B.300d.txt',False,True,'./Emb_Mat.mat')\n",
    "f=open(\"Emb_Mat.mat\",\"rb\")\n",
    "embedding_matrix=pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       exp\n",
      "1       sug\n",
      "2       exp\n",
      "3       sug\n",
      "4       sta\n",
      "5       sta\n",
      "6       que\n",
      "7       exp\n",
      "8       sta\n",
      "9       thr\n",
      "10      sta\n",
      "11      sug\n",
      "12      sug\n",
      "13      sta\n",
      "14      sta\n",
      "15      exp\n",
      "16      sta\n",
      "17      sta\n",
      "18      exp\n",
      "19      sta\n",
      "20      que\n",
      "21      thr\n",
      "22      exp\n",
      "23      exp\n",
      "24      exp\n",
      "25      exp\n",
      "26      exp\n",
      "27      sta\n",
      "28      sta\n",
      "29      sta\n",
      "       ... \n",
      "6808    oth\n",
      "6809    oth\n",
      "6810    que\n",
      "6811    exp\n",
      "6812    sug\n",
      "6813    sta\n",
      "6814    sta\n",
      "6815    exp\n",
      "6816    sta\n",
      "6817    exp\n",
      "6818    exp\n",
      "6819    exp\n",
      "6820    sta\n",
      "6821    que\n",
      "6822    sta\n",
      "6823    exp\n",
      "6824    que\n",
      "6825    exp\n",
      "6826    exp\n",
      "6827    que\n",
      "6828    sug\n",
      "6829    oth\n",
      "6830    oth\n",
      "6831    oth\n",
      "6832    que\n",
      "6833    sta\n",
      "6834    sta\n",
      "6835    sug\n",
      "6836    que\n",
      "6837    sta\n",
      "Name: Tweet_label, Length: 6838, dtype: object\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def create_label(label):\n",
    "\t\n",
    "    Y=[]\n",
    "    for i in label:\n",
    "    \txxx=np.empty(int(len(lbl_dict)))\n",
    "    \txxx.fill(0)\n",
    "    \tj=lbl_dict.get(i)\n",
    "    \txxx[j]=1\n",
    "    \tY.append(xxx)\n",
    "    return Y\n",
    "\n",
    "label = train_label\n",
    "Y_train = create_label(label)\n",
    "print(label)\n",
    "y_train=np.array([i for i in Y_train])\n",
    "\n",
    "embedding_dim = 300\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_func = lambda: get_model(\n",
    "    embedding_matrix,\n",
    "    sentences_length,\n",
    "    dropout_rate,\n",
    "    recurrent_units,\n",
    "    dense_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_train\n",
    "y=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': 0, 'anticipation': 1, 'disgust': 2, 'fear': 3, 'love': 4, 'joy': 5, 'sadness': 6, 'surprise': 7, 'trust': 8}\n",
      "[0. 0. 0. 0. 1. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "lbl_dict={'anger':0 , 'anticipation':1 , 'disgust':2, 'fear':3 , 'love':4 , 'joy':5 , 'sadness':6 , 'surprise':7 , 'trust':8 }\n",
    "print(lbl_dict)\n",
    "\n",
    "#Multi-hot encoding\n",
    "\t\n",
    "Y_train_emo=[]\n",
    "for i in data_train.index:\n",
    "    xxx=np.empty(int(len(lbl_dict)))\n",
    "    xxx.fill(0)\n",
    "    if(data_train['anger'][i]==1):\n",
    "        xxx[lbl_dict['anger']]=1      \n",
    "    if(data_train['anticipation'][i]==1):\n",
    "        xxx[lbl_dict['anticipation']]=1       \n",
    "    if(data_train['disgust'][i]==1):\n",
    "        xxx[lbl_dict['disgust']]=1      \n",
    "    if(data_train['anticipation'][i]==1):\n",
    "        xxx[lbl_dict['anticipation']]=1       \n",
    "    if(data_train['fear'][i]==1):\n",
    "        xxx[lbl_dict['fear']]=1\n",
    "    if(data_train['love'][i]==1):\n",
    "        xxx[lbl_dict['love']]=1\n",
    "    if(data_train['joy'][i]==1):\n",
    "        xxx[lbl_dict['joy']]=1\n",
    "    if(data_train['sadness'][i]==1):\n",
    "        xxx[lbl_dict['sadness']]=1\n",
    "    if(data_train['surprise'][i]==1):\n",
    "        xxx[lbl_dict['surprise']]=1\n",
    "    if(data_train['trust'][i]==1):\n",
    "        xxx[lbl_dict['trust']]=1\n",
    "    Y_train_emo.append(xxx) \n",
    "    \n",
    "y_train_emo=np.array([i for i in Y_train_emo])\n",
    "print(y_train_emo[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_model(embedding_matrix, sequence_length, dropout_rate, recurrent_units, dense_size):\n",
    "input1 = Input(shape=(sequences_length,))\n",
    "embed_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input1)\n",
    "embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "x = Bidirectional(GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(embed_layer)\n",
    "capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings, share_weights=True)(x)\n",
    "capsule = Flatten()(capsule)\n",
    "capsule = Dropout(dropout_p)(capsule)\n",
    "output = Dense(13, activation='sigmoid',name = 'dialog_acts')(capsule)\n",
    "output_emo=Dense(9, activation='sigmoid', name = 'emotion')(capsule)\n",
    "model = Model(inputs=input1, outputs=[output, output_emo])\n",
    "model.compile(\n",
    "        loss=['categorical_crossentropy','categorical_crossentropy'],\n",
    "        optimizer='adam',\n",
    "        metrics={'dialog_acts':'accuracy'}, loss_weights = [1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 79)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 79, 300)      2052300     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_5 (SpatialDro (None, 79, 300)      0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 79, 256)      329472      spatial_dropout1d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "capsule_5 (Capsule)             (None, 10, 16)       40960       bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 160)          0           capsule_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 160)          0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dialog_acts (Dense)             (None, 13)           2093        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "emotion (Dense)                 (None, 9)            1449        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,426,274\n",
      "Trainable params: 373,974\n",
      "Non-trainable params: 2,052,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5470 samples, validate on 1368 samples\n",
      "Epoch 1/10\n",
      "5470/5470 [==============================] - 92s 17ms/step - loss: 5.3513 - dialog_acts_loss: 1.6410 - emotion_loss: 3.7102 - dialog_acts_acc: 0.4223 - val_loss: 5.3420 - val_dialog_acts_loss: 1.6956 - val_emotion_loss: 3.6464 - val_dialog_acts_acc: 0.4020\n",
      "Epoch 2/10\n",
      "5470/5470 [==============================] - 84s 15ms/step - loss: 4.9567 - dialog_acts_loss: 1.3007 - emotion_loss: 3.6560 - dialog_acts_acc: 0.4296 - val_loss: 5.3510 - val_dialog_acts_loss: 1.7078 - val_emotion_loss: 3.6432 - val_dialog_acts_acc: 0.4020\n",
      "Epoch 3/10\n",
      "5470/5470 [==============================] - 85s 15ms/step - loss: 4.9256 - dialog_acts_loss: 1.2741 - emotion_loss: 3.6515 - dialog_acts_acc: 0.4305 - val_loss: 5.3425 - val_dialog_acts_loss: 1.7008 - val_emotion_loss: 3.6418 - val_dialog_acts_acc: 0.4020\n",
      "Epoch 4/10\n",
      "5470/5470 [==============================] - 86s 16ms/step - loss: 4.9108 - dialog_acts_loss: 1.2642 - emotion_loss: 3.6466 - dialog_acts_acc: 0.4309 - val_loss: 5.3535 - val_dialog_acts_loss: 1.7144 - val_emotion_loss: 3.6391 - val_dialog_acts_acc: 0.4020\n",
      "Epoch 5/10\n",
      "5470/5470 [==============================] - 76s 14ms/step - loss: 4.9055 - dialog_acts_loss: 1.2583 - emotion_loss: 3.6473 - dialog_acts_acc: 0.4303 - val_loss: 5.3461 - val_dialog_acts_loss: 1.7036 - val_emotion_loss: 3.6425 - val_dialog_acts_acc: 0.4020\n",
      "Epoch 6/10\n",
      "5470/5470 [==============================] - 72s 13ms/step - loss: 4.9066 - dialog_acts_loss: 1.2597 - emotion_loss: 3.6470 - dialog_acts_acc: 0.4305 - val_loss: 5.3563 - val_dialog_acts_loss: 1.7174 - val_emotion_loss: 3.6389 - val_dialog_acts_acc: 0.4020\n",
      "Epoch 7/10\n",
      "5470/5470 [==============================] - 80s 15ms/step - loss: 4.9019 - dialog_acts_loss: 1.2568 - emotion_loss: 3.6452 - dialog_acts_acc: 0.4307 - val_loss: 5.3453 - val_dialog_acts_loss: 1.7060 - val_emotion_loss: 3.6392 - val_dialog_acts_acc: 0.4020\n",
      "Epoch 8/10\n",
      "5470/5470 [==============================] - 78s 14ms/step - loss: 4.9041 - dialog_acts_loss: 1.2579 - emotion_loss: 3.6462 - dialog_acts_acc: 0.4307 - val_loss: 5.2994 - val_dialog_acts_loss: 1.6625 - val_emotion_loss: 3.6369 - val_dialog_acts_acc: 0.4020\n",
      "Epoch 9/10\n",
      "5470/5470 [==============================] - 73s 13ms/step - loss: 4.8985 - dialog_acts_loss: 1.2538 - emotion_loss: 3.6447 - dialog_acts_acc: 0.4307 - val_loss: 5.3121 - val_dialog_acts_loss: 1.6719 - val_emotion_loss: 3.6402 - val_dialog_acts_acc: 0.4020\n",
      "Epoch 10/10\n",
      "5470/5470 [==============================] - 83s 15ms/step - loss: 4.8994 - dialog_acts_loss: 1.2537 - emotion_loss: 3.6456 - dialog_acts_acc: 0.4307 - val_loss: 5.3481 - val_dialog_acts_loss: 1.7114 - val_emotion_loss: 3.6367 - val_dialog_acts_acc: 0.4020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x200e73b9550>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X], [y, y_train_emo], epochs=10, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
